<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="Sparsh: Self-supervised touch representations for vision-based tactile sensing">
  <meta name="keywords" content="Sparsh, tactile, sensing, DIGIT, GelSight, GelSight mini, Self-supervised learning">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Sparsh | Self-supervised touch representations for vision-based tactile sensing</title>

  <!-- Thumbnail for social media sharing -->
  <meta property="og:image" content="media/thumbnail.png">

  <!-- Favicon -->
  <link rel="icon" href="media/thumbnail.png" type="image/png">

  <script>
    window.dataLayer = window.dataLayer || [];
  </script>

  <script>
    function updateInTheWild() {
      var task = document.getElementById("inthewild-video-menu").value;

      console.log("updateInTheWild", task)

      var video = document.getElementById("inthewild-video");
      video.src = "media/videos/" +
                  task +
                  ".m4v"
      video.play();
    }

    function updateFieldVis() {
      var task = document.getElementById("field-video-menu").value;

      console.log("updateBimanual", task)

      var video = document.getElementById("field-video");
      video.src = "media/videos/" +
                  task +
                  ".mp4"
      video.play();
    }

    function updateClothes() {
      var task = document.getElementById("clothes-video-menu").value;

      console.log("updateclothes", task)

      var img = document.getElementById("clothes-img");
      img.src = "media/fold-strategies/" +
                  task +
                  ".jpeg"

      var video = document.getElementById("clothes-video");
      video.src = "media/videos/fold-" +
                  task +
                  ".mp4"
      video.play();
    }
  </script>


  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">

  <link rel="stylesheet" href="./static/source_serif_4.css">
  <link rel="stylesheet" href="./static/source_sans_3.css">
  <link rel="stylesheet" href="./static/academicons.min.css">
  <link rel="stylesheet" href="./static/fontawesome/css/fontawesome.css">
  <link rel="stylesheet" href="./static/fontawesome/css/brands.css">
  <link rel="stylesheet" href="./static/fontawesome/css/light.css">


  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body onload="updateInTheWild();updateBimanual();">

<section class="hero">
  <div class="hero-body">
    <div class="container is-fullhd">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title"> Sparsh: Self-supervised touch representations <br>
            for vision-based tactile sensing</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a target="_blank" href="https://carolinahiguera.github.io/">Carolina Higuera</a><sup>1,2*</sup>,
            </span>
            <span class="author-block">
              <a target="_blank" href="https://akashsharma02.github.io/">Akash Sharma</a><sup>1,3*</sup>,
            </span>
            <span class="author-block">
              <a target="_blank" href="https://github.com/chaitanyantr">Chaitanya Krishna Bodduluri</a><sup>1</sup>,
            </span>
            <span class="author-block">
              <a target="_blank" href="https://scholar.google.com/citations?user=3PJeg1wAAAAJ&hl=en">Taosha Fan</a><sup>1</sup>,
            </span>
            <span class="author-block">
              <a target="_blank" href="https://palanc.github.io/">Patrick Lancaster</a><sup>1</sup>,
            </span>
            <span class="author-block">
              <a target="_blank" href="https://www.linkedin.com/in/mrinalkalakrishnan/">Mrinal Kalakrishnan</a><sup>1</sup>,
            </span>
            <span class="author-block">
              <a target="_blank" href="https://www.cs.cmu.edu/~kaess/">Michael Kaess</a><sup>3</sup>
            </span>
            <span class="author-block">
              <a target="_blank" href="https://homes.cs.washington.edu/~bboots/">Byron Boots</a><sup>2</sup>
            </span>
            <span class="author-block">
              <a target="_blank" href="https://www.linkedin.com/in/tingfan-wu-5359669/">Tingfan Wu</a><sup>1</sup>
            </span>
            <span class="author-block">
              <a target="_blank" href="https://www.mustafamukadam.com/">Mustafa Mukadam</a><sup>1</sup>
            </span>
          </div>
          <div class="is-size-5 affiliation">
            <sup>1</sup>FAIR at Meta,
            <sup>2</sup>University of Washington,
            <sup>3</sup>Carnegie Mellon University,
          </div>
          <br>
          <div class="affiliation-note">
            <sup>*</sup> indicates equal contributions
          </div>
          <div class="button-container">
            <a href="./sparsh.pdf" target="_blank" class="button"><i class="fa-light fa-file"></i>&emsp14;PDF</a>
            <a href="todo" target="_blank" class="button"><i class="ai ai-arxiv"></i>&emsp14;arXiv</a>
            <a href="https://youtu.be/CHGLXpahQpU?si=dHdm4zI41u1nncZM" target="_blank" class="button"><i class="fa-light fa-film"></i>&emsp14;Video</a>
            <a href="https://github.com/huangwl18/ReKep" target="_blank" class="button"><i class="fa-light fa-code"></i>&emsp14;Code</a>
            <a href="https://todo" target="_blank" class="button"><i class="fa-light fa-database"></i>&emsp14;Dataset</a>
        </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="hero teaser">
  <div class="container is-max-widescreen">
    <div class="hero-body">
      <div class="container">
        <div class="columns is-vcentered  is-centered">
          <!-- <video id="teaser" autoplay muted loop controls height="100%" width="100%"> -->
          <!--   <source src="media/videos/teaser.mp4" -->
          <!--           type="video/mp4"> -->
          <!-- </video> -->
            <img src="media/figures/teaser.png" class="teaser-image" />
          </br>
        </div>
        <br>
        <h3 class="subtitle has-text-justified">
        We present <span class="model">Sparsh</span>, a family of general touch representations, and build <span class="model">TacBench</span>,
        a standardized benchmark consisting of five touch-centric tasks (<span class='model'>[T1]</span>-<span class="model">[T6]</span>)
        covering prominent problems in vision-based tactile sensing. In evaluation (middle), we find <span class="model">Sparsh</span> trained via self-supervision on
        a dataset of 460k+ tactile images to generalize across tasks (right) and sensors (left) outperforming task and sensor specific
        models (<span class='model'>E2E</span>). Results of <span class="model">[T1]</span>- <span class="model">[T5]</span> shown are trained with 33% of the labeled data, except <span class='model'>[T5]</span>, which is trained with 50% of demonstration data.
        </h3>
      </div>
    </div>
  </div>

<div class="container is-max-widescreen">
<div class="columns is-centered has-text-centered">
  <div class="column is-four-fifths">
    <h2 class="title is-3">Abstract</h2>
    <div class="content has-text-justified">
    <p>
        In this work, we introduce general purpose touch representations for the increasingly accessible
        class of vision-based tactile sensors. Such sensors have led to many recent advances in robot
        manipulation as they markedly complement vision, yet solutions today often rely on task
        and sensor specific handcrafted perception models. Collecting real data at scale with task
        centric ground truth labels, like contact forces and slip, is a challenge further compounded
        by sensors of various form factor differing in aspects like lighting and gel markings.
    </p>

    <p>
        To tackle this we turn to self-supervised learning (SSL) that has demonstrated remarkable
        performance in computer vision. We present <span class="model">Sparsh</span>, a family of SSL models
        that can support various vision-based tactile sensors, alleviating the need for custom labels
        through pre-training on 460k+ tactile images with masking and self-distillation in pixel and latent spaces.
        We also build <span class="model">TacBench</span>, to facilitate standardized benchmarking across sensors and models,
        comprising of six tasks ranging from comprehending tactile properties to enabling physical
        perception and manipulation planning. In evaluations, we find that SSL pre-training for
        touch representation outperforms task and sensor-specific end-to-end training by 93.4%
        on average over <span class='model'>TacBench</span>, and <span class="model">Sparsh (DINO)</span> and
        <span class="model">Sparsh (IJEPA)</span> are the most competitive,
        indicating the merits of learning in latent space for tactile images.
    </p>
    </div>
  </div>
</div>



<hr class="rounded">
<div class="rows">
  <h2 class="title is-3">Walkthrough Video</h2>
  <div class="publication-video">
      <iframe src="https://www.youtube.com/embed/CHGLXpahQpU?si=dHdm4zI41u1nncZM" allow="autoplay; encrypted-media" allowfullscreen></iframe>
  </div>
</div>


<!-- <hr class="rounded"> -->
<!-- <div class="rows"> -->
<!--   <h2 class="title is-3">Overview of ReKep</h2> -->
<!--   <img src="media/figures/method.jpg" class="method-image" /> -->
<!--   <p class="content has-text-justified">Given RGB-D observation and free-form language instruction, DINOv2 -->
<!--     is used to propose keypoint candidates on fine-grained meaningful regions in the scene. The image overlaid -->
<!--     with keypoints and the instruction are fed into GPT-4o to generate a series of ReKep constraints as python -->
<!--     programs that specify desired relations between keypoints at different stages of the task and any -->
<!--     requirement on the transitioning behaviors. Finally, a constrained optimization solver is used to obtain a -->
<!--     dense sequence of end-effector actions in SE(3), subject to the generated constraints. -->
<!--     <b>The entire pipeline does not involve any additional training or any task-specific data.</b> -->
<!--   </p> -->
<!-- </div> -->

<hr class="rounded">
<div class="rows">
  <h2 class="title is-3">Normal and shear field decoding</h2>

  <p class="content has-text-justified">Using <span class="model">Sparsh</span>, you can obtain Normal and Shear fields from a tactile history window.
    These could be used fed into policies for robot manipulation tasks or used for state estimation.
    </p>

    <div class="columns">
      <div class="column has-text-centered">
          <div class="select is-rounded">
            <select id="field-video-menu" onchange="updateFieldVis()">
            <option value="tomato_dino" selected="selected">Tomato Can</option>
            <option value="mug_dino">Mug</option>
            <option value="sugar_dino">Sugar Box</option>
            <option value="sphere_dino">Sphere 4mm</option>
            </select>
          </div>
      </div>
    </div>

    <div class="columns">
      <div class="column has-text-centered">
        <p style="text-align:center;">
          <video id="field-video" width="100%" height="100%" controls autoplay loop muted>
            <source src="media/videos/tomato_dino.mp4" type="video/mp4">
          </video>
        </p>
      </div>
    </div>

</div>

<hr class="rounded">
<div class="rows">
  <h2 class="title is-3">Pose estimation</h2>
  <p class="content has-text-justified">
    <span class="model">Sparsh</span> representations cab be used to estimate SE(2) transformations of the object relative to the sensor. We follow the regression-by-classification paradigm, where relative object poses are binned into a grid.
  </p>
  <div class="columns">
    <div class="column has-text-centered">
      <video id="dist1"
        controls autoplay loop muted
        width="100%">
        <source src="media/videos/dino_pose.mp4" 
        type="video/mp4">
      </video>
    </div>
  </div>
</div>

<hr class="rounded">

<div class="rows">
  <h2 class="title is-3">Bead Maze</h2>
  <p class="content has-text-justified">
    <span class="model">Sparsh</span> representations can enable planning for manipulation. We adapt the Bead Maze task to robot policy learning, where the goal is to guide a bead from one end to another following the path of the wire. We use Diffusion Policy to train policies from a set of teleop demonstrations on different maze patterns, where the action space is delta joint angles for the robot arm.


  </p>
    <div class="columns">
      <div class="column has-text-centered">
        <div class="select is-rounded">
          <select id="inthewild-video-menu" onchange="updateInTheWild()">
            <option value="stow-book" selected="selected">Stow Book</option>
          <option value="pour-tea">Pour Tea</option>
          <option value="tape-box">Tape Box</option>
          <option value="recycle-can">Recycle Can</option>
          </select>
        </div>
      </div>

    </div>

    <div class="columns">
      <div class="column has-text-centered">
        <p style="text-align:center;">
          <video id="inthewild-video" width="100%" height="100%" controls autoplay loop muted>
            <source src="media/videos/pour-tea-dist.m4v" type="video/mp4">
          </video>
        </p>
      </div>
    </div>
</div>



<hr class="rounded">

<!-- <div class="rows">
  <h2 class="title is-3">Folding Clothes with Novel Strategies</h2>
    <p class="content has-text-justified">
      The system can also generate novel strategies for the same task but under different scenarios.
      Specifically, we investigate whether the same system can fold different types of clothing items.
      Interestingly, we observe drastically different strategies across the clothing categories, many of which align with how humans might fold each garment.
      Select a garment to see its folding strategy and its video.
      The coloring of the keypoints indicates the folding order, where red keypoints are aligned first and the blue keypoints are aligned subsequently.
    </p>
    <div class="columns">
      <div class="column has-text-centered">
        <div class="select is-rounded">
          <select id="clothes-video-menu" onchange="updateClothes()">
            <option value="sweater" selected="selected">Sweater</option>
          <option value="shirt">Shirt</option>
          <option value="hoodie">Hoodie</option>
          <option value="vest">Vest</option>
          <option value="dress">Dress</option>
          <option value="pants">Pants</option>
          <option value="shorts">Shorts</option>
          <option value="scarf">Scarf</option>
          </select>
        </div>
      </div>

    </div>

    <div class="columns">
      <div class="column has-text-centered">
        <p style="text-align:center;">
          <img id="clothes-img" width="60%" src="media/fold-strategies/sweater.jpeg" class="method-image" />
        </p>
      </div>
      <div class="column has-text-centered">
        <p style="text-align:center;">
          <video id="clothes-video" width="100%" height="100%" controls autoplay loop muted>
            <source src="media/videos/fold-sweater.mp4" type="video/mp4">
          </video>
        </p>
      </div>
    </div>
</div> -->

<hr class="rounded">
<h2 class="title is-3">Acknowledgments</h2>
<p>
  We thank Ishan Misra, Mahmoud Assran for insightful discussions on SSL for vision that informed this work, and Changhao Wang, Dhruv Batra, Jitendra Malik, Luis Pineda, Tess Hellebrekers for helpful discussions on the research. <br> <br>
  We base our implementations on the following repositories: <br>
  - <a href="https://github.com/real-stanford/diffusion_policy">Diffusion Policy</a> <br>
  - <a href="https://github.com/facebookresearch/ijepa" target="_blank">IJEPA</a> <br>
  - <a href="https://github.com/facebookresearch/jepa" target="_blank">VJEPA</a> <br>
  - <a href="https://github.com/facebookresearch/dino/" target="_blank">DINO</a> <br>
  - <a href="https://github.com/facebookresearch/mae" target="_blank">MAE</a> <br>
</p>



  
</p>
<hr class="rounded">
<h2 class="title is-3">BibTeX</h2>
<p class="bibtex">
    @article{higuera2024sparsh, <br>
    &nbsp;&nbsp;title = {Sparsh: Self-supervised touch representations for vision-based tactile sensing}, <br>
    &nbsp;&nbsp;author = {Carolina Higuera and Akash Sharma and Chaithanya Krishna Bodduluri and Taosha Fan and Patrick Lancaster and Mrinal Kalakrishnan and Michael Kaess and Byron Boots and Mike Lambeta and Tingfan Wu and Mustafa Mukadam}, <br>
    &nbsp;&nbsp;booktitle = {8th Annual Conference on Robot Learning}, <br>
    &nbsp;&nbsp;year = {2024}, <br>
    &nbsp;&nbsp;url = {https://openreview.net/forum?id=xYJn2e1uu8} <br>
    } <br>
</p>

</section>
</div>

<footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column">
        <div class="content has-text-centered">
          <p>
            Website template borrowed from <a href="https://nerfies.github.io">Nerfies</a>,
            <a href="https://dex-cap.github.io">DexCap</a>,
            and <a href="https://rekep-robot.github.io">ReKep</a>.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>


</body>
</html>
