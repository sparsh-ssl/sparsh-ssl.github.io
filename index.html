<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="Sparsh: Self-supervised touch representations for vision-based tactile sensing">
  <meta name="keywords" content="Sparsh, tactile, sensing, DIGIT, GelSight, GelSight mini, Self-supervised learning">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Sparsh | Self-supervised touch representations for vision-based tactile sensing</title>

  <!-- Thumbnail for social media sharing -->
  <meta property="og:image" content="media/thumbnail.png">

  <!-- Favicon -->
  <link rel="icon" href="media/thumbnail.png" type="image/png">

  <script>
    window.dataLayer = window.dataLayer || [];
  </script>

  <script>
    function updateInTheWild() {
      var task = document.getElementById("inthewild-video-menu").value;

      console.log("updateInTheWild", task)

      var video = document.getElementById("inthewild-video");
      video.src = "media/videos/" +
                  task +
                  ".mp4"
      video.play();
    }

    function updateFieldVis() {
      var task = document.getElementById("field-video-menu").value;

      console.log("updateBimanual", task)

      var video = document.getElementById("field-video");
      video.src = "media/videos/" +
                  task +
                  ".mp4"
      video.play();
    }

    function updateClothes() {
      var task = document.getElementById("clothes-video-menu").value;

      console.log("updateclothes", task)

      var img = document.getElementById("clothes-img");
      img.src = "media/fold-strategies/" +
                  task +
                  ".jpeg"

      var video = document.getElementById("clothes-video");
      video.src = "media/videos/fold-" +
                  task +
                  ".mp4"
      video.play();
    }
  </script>


  <link rel="stylesheet" href="./static/fonts/cmu_bright/cmun-bright.css">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">

  <link rel="stylesheet" href="./static/source_serif_4.css">
  <link rel="stylesheet" href="./static/source_sans_3.css">
  <link rel="stylesheet" href="./static/academicons.min.css">
  <link rel="stylesheet" href="./static/fontawesome/css/fontawesome.css">
  <link rel="stylesheet" href="./static/fontawesome/css/brands.css">
  <link rel="stylesheet" href="./static/fontawesome/css/light.css">


  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body onload="updateInTheWild();updateBimanual();">

<section class="hero">
  <div class="hero-body">
    <div class="container is-fullhd">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title"> Sparsh: Self-supervised touch representations <br>
            for vision-based tactile sensing</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a target="_blank" href="https://carolinahiguera.github.io/">Carolina Higuera</a><sup>1,2*</sup>,
            </span>
            <span class="author-block">
              <a target="_blank" href="https://akashsharma02.github.io/">Akash Sharma</a><sup>1,3*</sup>,
            </span>
            <span class="author-block">
              <a target="_blank" href="https://github.com/chaitanyantr">Chaitanya Krishna Bodduluri</a><sup>1</sup>,
            </span>
            <span class="author-block">
              <a target="_blank" href="https://scholar.google.com/citations?user=3PJeg1wAAAAJ&hl=en">Taosha Fan</a><sup>1</sup>,
            </span>
            <span class="author-block">
              <a target="_blank" href="https://palanc.github.io/">Patrick Lancaster</a><sup>1</sup>,
            </span>
            <span class="author-block">
              <a target="_blank" href="https://www.linkedin.com/in/mrinalkalakrishnan/">Mrinal Kalakrishnan</a><sup>1</sup>,
            </span>
            <span class="author-block">
              <a target="_blank" href="https://www.cs.cmu.edu/~kaess/">Michael Kaess</a><sup>3</sup>
            </span>
            <span class="author-block">
              <a target="_blank" href="https://homes.cs.washington.edu/~bboots/">Byron Boots</a><sup>2</sup>
            </span>
            <span class="author-block">
              <a target="_blank" href="https://www.linkedin.com/in/tingfan-wu-5359669/">Tingfan Wu</a><sup>1</sup>
            </span>
            <span class="author-block">
              <a target="_blank" href="https://www.mustafamukadam.com/">Mustafa Mukadam</a><sup>1</sup>
            </span>
          </div>
          <div class="is-size-5 affiliation">
            <sup>1</sup>FAIR at Meta,
            <sup>2</sup>University of Washington,
            <sup>3</sup>Carnegie Mellon University,
          </div>
          <br>
          <div class="affiliation-note">
            <sup>*</sup> indicates equal contribution
          </div>
          <div class="button-container">
            <a href="./sparsh.pdf" target="_blank" class="button"><i class="fa-light fa-file"></i>&emsp14;PDF</a>
            <a href="todo" target="_blank" class="button"><i class="ai ai-arxiv"></i>&emsp14;arXiv</a>
            <a href="https://youtu.be/CHGLXpahQpU?si=dHdm4zI41u1nncZM" target="_blank" class="button"><i class="fa-light fa-film"></i>&emsp14;Video</a>
            <a href="https://github.com/facebookresearch/sparsh-ssl.git" target="_blank" class="button"><i class="fa-light fa-code"></i>&emsp14;Code</a>
            <a href="https://todo" target="_blank" class="button"><i class="fa-light fa-database"></i>&emsp14;Dataset</a>
        </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="hero teaser">
  <div class="container is-max-widescreen">
    <div class="hero-body">
      <div class="container">
        <div class="columns is-vcentered  is-centered">
            <img src="media/figures/teaser2.png" class="teaser-image" />
          </br>
        </div>
        <br>
        <h3 class="subtitle has-text-justified">
        We present <span class="model">Sparsh</span>, a family of general touch representations, and <span class="model">TacBench</span>,
        a standardized benchmark consisting of six touch-centric tasks (<span class='model'>[T1]</span>-<span class="model">[T6]</span>)
        covering prominent problems in vision-based tactile sensing. In evaluation (middle), we find <span class="model">Sparsh</span> trained with self-supervision on
        a dataset of 460k+ tactile images to generalize across tasks (right) and sensors (left) outperforming task and sensor specific
        models (<span class='model'>E2E</span>). <span class="model">[T1]</span>- <span class="model">[T5]</span> and <span class="model">[T6]</span> are trained with 33% and 50% of the labeled data respectively.
        </h3>
      </div>
    </div>
  </div>

<div class="container is-max-widescreen">
<div class="columns is-centered has-text-centered">
  <div class="column is-four-fifths">
    <h2 class="title is-3">Abstract</h2>
    <div class="content has-text-justified">
    <p>
        In this work, we introduce general purpose touch representations for the increasingly accessible 
        class of vision-based tactile sensors. Such sensors have led to many recent advances in robot
        manipulation as they markedly complement vision, yet solutions today often rely on task 
        and sensor specific handcrafted perception models. Collecting real data at scale with task 
        centric ground truth labels, like contact forces and slip, is a challenge further compounded 
        by sensors of various form factor differing in aspects like lighting and gel markings. 
    </p>
    <p>
        To tackle this we turn to self-supervised learning (SSL) that has demonstrated remarkable 
        performance in computer vision. We present <span class="model">Sparsh</span>, a family of SSL models 
        that can support various vision-based tactile sensors, alleviating the need for custom labels 
        through pre-training on 460k+ tactile images with masking and self-distillation in pixel and latent spaces. 
        We also build <span class="model">TacBench</span>, to facilitate standardized benchmarking across sensors and models, 
        comprising of six tasks ranging from comprehending tactile properties to enabling physical 
        perception and manipulation planning. In evaluations, we find that SSL pre-training for 
        touch representation outperforms task and sensor-specific end-to-end training by 95.1%
        on average over <span class="model">TacBench</span>, and <span class="model">Sparsh (DINO)</span>
        <span class="model">Sparsh (IJEPA)</span> are the most competitive, 
        indicating the merits of learning in latent space for tactile images. 
    </p>
    </div>
  </div>
</div>



<hr class="rounded">
<div class="rows">
  <h2 class="title is-3">Walkthrough Video</h2>
  <div class="publication-video">
      <iframe src="https://www.youtube.com/embed/CHGLXpahQpU?si=dHdm4zI41u1nncZM" allow="autoplay; encrypted-media" allowfullscreen></iframe>
  </div>
</div>


<hr class="rounded">
<div class="rows">
  <h2 class="title is-3">Overview of Sparsh</h2>
  <img src="media/figures/ssl.png" class="method-image" />
  <p class="content has-text-justified">
    To address the scarcity of labeled and even unlabeled data in the tactile domain, 
    we curate together new and existing vision-based tactile sensor datasets totaling ~661k samples. 
    Further, we investigate self-supervised learning approaches, such as Masked Autoencoding (MAE), Self-Distillation (DINO and DINOv2), 
    and Joint embedding Prediction (JEPA) to train the <span class="model">Sparsh</span> family of foundation models for general purpose touch representations. We find that background subtraction and tokenization 
    of a small temporal window (~80 ms) of tactile images are crucial for learning generalizable and effective representations. 
    We then evaluate the general-purpose utility of these representations on <span class="model">TacBench</span>, a benchmark of six tasks that span tactile perception.
  </p>
</div>

<hr class="rounded">
<div class="rows">
  <h2 class="title is-3">Normal and shear field decoding</h2>

  <p class="content has-text-justified">
    Using <span class="model">Sparsh</span>, one can obtain in realtime normal and shear fields from a stream of tactile sensor images
    in realtime. To predict these fields, we train a DPT decoder given (frozen) latent representations from <span class="model">Sparsh</span> 
    via photometric losses computed by warping tactile image frames using the predicted normal and shear forces.
    These fields can be used to estimate contact location and normals and could be fed into policies for robot manipulation tasks or used for state estimation.
    </p>

    <div class="columns">
      <div class="column has-text-centered">
          <div class="select is-rounded">
            <select id="field-video-menu" onchange="updateFieldVis()">
            <option value="tomato_dino" selected="selected">Tomato Can</option>
            <option value="mug_dino">Mug</option>
            <option value="sugar_dino">Sugar Box</option>
            <option value="sphere_dino">Sphere 4mm</option>
            </select>
          </div>
      </div>
    </div>

    <div class="columns">
      <div class="column has-text-centered">
        <p style="text-align:center;">
          <video id="field-video" width="100%" height="100%" controls autoplay loop muted>
            <source src="media/videos/tomato_dino.mp4" type="video/mp4">
          </video>
        </p>
      </div>
    </div>

</div>

<hr class="rounded">
<div class="rows">
  <h2 class="title is-3">Pose estimation</h2>
  <p class="content has-text-justified">
    We also show that <span class="model">Sparsh</span> representations capture relative object pose information. 
    To investigate this, we follow the <em>regression-by-classification</em> paradigm. Specifically, we probe the model 
    with an <em>attentive probe</em>, to estimate SE(2) transformations of the object relative to the sensor. 
    Further results can be found in our paper.
  </p>
  <div class="columns">
    <div class="column has-text-centered">
      <video id="dist1"
        controls autoplay loop muted
        width="100%">
        <source src="media/videos/dino_pose.mp4" 
        type="video/mp4">
      </video>
    </div>
  </div>
</div>

<hr class="rounded">

<div class="rows">
  <h2 class="title is-3">Bead Maze</h2>
  <p class="content has-text-justified">
    <span class="model">Sparsh</span> representations can enable planning for manipulation. 
    We adapt the Bead Maze task to robot policy learning, where the goal is to guide a bead from one end to another 
    following the path of the wire. Here, we use Diffusion Policy to train policies from a set of teleop demonstrations 
    on different maze patterns. Given the tactile images, the policy predicts the joint angles for the Franka robot arm. 
    We find that in general, policies trained with <span class="model">Sparsh</span> representations perform slightly better 
    than training encoders end-to-end.
  </p>
    <div class="columns">
      <div class="column has-text-centered">
        <div class="select is-rounded">
          <select id="inthewild-video-menu" onchange="updateInTheWild()">
            <option value="dino_full_28052024_16x_overlay" selected="selected">DINO</option>
            <option value="ijepa_full_28052024_16x_overlay">IJEPA</option>
            <option value="mae_full_28052024_16x_overlay">MAE</option>
            <option value="e2e_full_28052024_16x_overlay">End to End</option>
            <option value="ijepa_thirdsection_sdr_16x_overlay">IJEPA (last 1/3 maze)</option>
            <option value="e2e_thirdsection_sdr_16x_overlay">End to End (last 1/3 maze)</option>
          </select>
        </div>
      </div>

    </div>

    <div class="columns">
      <div class="column has-text-centered">
        <p style="text-align:center;">
          <video id="inthewild-video" width="100%" height="100%" controls autoplay loop muted>
            <source src="media/videos/pour-tea-dist.m4v" type="video/mp4">
          </video>
        </p>
      </div>
    </div>
</div>

<hr class="rounded">
<h2 class="title is-3">Acknowledgments</h2>
<p>
  We thank Ishan Misra, Mahmoud Assran for insightful discussions on SSL for vision that informed this work, and Changhao Wang, Dhruv Batra, Jitendra Malik, Luis Pineda, Tess Hellebrekers for helpful discussions on the research. <br> <br>
  Our implementation is based on the following repositories: 
    <ul>
        <li><a href="https://github.com/real-stanford/diffusion_policy">Diffusion Policy</a></li>
        <li><a href="https://github.com/facebookresearch/ijepa" target="_blank">IJEPA</a></li>
        <li><a href="https://github.com/facebookresearch/jepa" target="_blank">VJEPA</a></li>
        <li><a href="https://github.com/facebookresearch/dino/" target="_blank">DINO</a></li>
        <li><a href="https://github.com/facebookresearch/mae" target="_blank">MAE</a></li>
    </ul>
</p>
  
<hr class="rounded">
<h2 class="title is-3">BibTeX</h2>
If you find our work useful, please consider citing our paper:
<p class="bibtex">
    @article{higuera2024sparsh, <br>
    &nbsp;&nbsp;title = {Sparsh: Self-supervised touch representations for vision-based tactile sensing}, <br>
    &nbsp;&nbsp;author = {Carolina Higuera and Akash Sharma and Chaithanya Krishna Bodduluri and Taosha Fan and Patrick Lancaster and Mrinal Kalakrishnan and Michael Kaess and Byron Boots and Mike Lambeta and Tingfan Wu and Mustafa Mukadam}, <br>
    &nbsp;&nbsp;booktitle = {8th Annual Conference on Robot Learning}, <br>
    &nbsp;&nbsp;year = {2024}, <br>
    &nbsp;&nbsp;url = {https://openreview.net/forum?id=xYJn2e1uu8} <br>
    } <br>
</p>



  
</section>
</div>

<footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column">
        <div class="content has-text-centered">
          <p>
            Website template borrowed from <a href="https://nerfies.github.io">Nerfies</a>,
            <a href="https://dex-cap.github.io">DexCap</a>,
            and <a href="https://rekep-robot.github.io">ReKep</a>.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>



  
</body>
</html>
